[#]: collector: (lujun9972)
[#]: translator: (MjSeven)
[#]: reviewer: (wxy)
[#]: publisher: (wxy)
[#]: url: (https://linux.cn/article-10852-1.html)
[#]: subject: (Getting started with social media sentiment analysis in Python)
[#]: via: (https://opensource.com/article/19/4/social-media-sentiment-analysis-python)
[#]: author: (Michael McCune  https://opensource.com/users/elmiko/users/jschlessman)

使用 Python 进行社交媒体情感分析入门
======

> 学习自然语言处理的基础知识并探索两个有用的 Python 包。

![](https://img.linux.net.cn/data/attachment/album/201905/14/002943t6udxhhcq1zoxu15.jpg)

自然语言处理（NLP）是机器学习的一种，它解决了口语或书面语言和计算机辅助分析这些语言之间的相关性。日常生活中我们经历了无数的 NLP 创新，从写作帮助和建议到实时语音翻译，还有口译。

本文研究了 NLP 的一个特定领域：情感分析。重点是确定输入语言的积极、消极或中性性质。本部分将解释 NLP 和情感分析的背景，并探讨两个开源的 Python 包。[第 2 部分][2]将演示如何开始构建自己的可扩展情感分析服务。

在学习情感分析时，对 NLP 有一个大体了解是有帮助的。本文不会深入研究数学本质。相反，我们的目标是阐明 NLP 中的关键概念，这些概念对于将这些方法实际结合到你的解决方案中至关重要。

### 自然语言和文本数据

合理的起点是从定义开始：“什么是自然语言？”它是我们人类相互交流的方式，沟通的主要方式是口语和文字。我们可以更进一步，只关注文本交流。毕竟，生活在 Siri、Alexa 等无处不在的时代，我们知道语音是一组与文本无关的计算。

### 数据前景和挑战

我们只考虑使用文本数据，我们可以对语言和文本做什么呢？首先是语言，特别是英语，除了规则还有很多例外，含义的多样性和语境差异，这些都可能使人类口译员感到困惑，更不用说计算机翻译了。在小学，我们学习文章和标点符号，通过讲母语，我们获得了寻找直觉上表示唯一意义的词的能力。比如，出现诸如 “a”、“the” 和 “or” 之类的文章，它们在 NLP 中被称为*停止词*，因为传统上 NLP 算法是在一个序列中找到这些词时意味着搜索停止。

由于我们的目标是自动将文本分类为情感类，因此我们需要一种以计算方式处理文本数据的方法。因此，我们必须考虑如何向机器表示文本数据。众所周知，利用和解释语言的规则很复杂，输入文本的大小和结构可能会有很大差异。我们需要将文本数据转换为数字数据，这是机器和数学的首选方式。这种转变属于*特征提取*的范畴。

在提取输入文本数据的数字表示形式后，一个改进可能是：给定一个文本输入体，为上面列出的文章确定一组向量统计数据，并根据这些数据对文档进行分类。例如，过多的副词可能会使撰稿人感到愤怒，或者过度使用停止词可能有助于识别带有内容填充的学期论文。诚然，这可能与我们情感分析的目标没有太大关系。

### 词袋

当你评估一个文本陈述是积极还是消极的时候，你使用哪些上下文来评估它的极性？（例如，文本中是否具有积极的、消极的或中性的情感）一种方式是隐含形容词：被称为 “disgusting”（恶心） 的东西被认为是消极的，但如果同样的东西被称为 “beautiful”（漂亮），你会认为它是积极的。从定义上讲，俗语给人一种熟悉感，通常是积极的，而脏话可能是敌意的表现。文本数据也可以包括表情符号，它带有固定的情感。

理解单个单词的极性影响为文本的<ruby>[词袋][3]<rt>bag-of-words</rt></ruby>（BoW）模型提供了基础。它分析一组单词或词汇表，并提取关于这些单词在输入文本中是否存在的度量。词汇表是通过处理已知极性的文本形成称为*标记的训练数据*。从这组标记数据中提取特征，然后分析特征之间的关系，并将标记与数据关联起来。

“词袋”这个名称说明了它的用途：即不考虑空间位置或上下文的的单个词。词汇表通常是由训练集中出现的所有单词构建的，训练后往往会被修剪。如果在训练之前没有清理停止词，那么停止词会因为其高频率和低语境而被移除。很少使用的单词也可以删除，因为缺乏为一般输入实例提供的信息。

但是，重要的是要注意，你可以（并且应该）进一步考虑单词在单个训练数据实例之外的情形，这称为<ruby>[词频][4]<rt>term frequency</rt></ruby>（TF）。你还应该考虑输入数据在所有训练实例中的单词计数，通常，出现在所有文档中的低频词更重要，这被称为<ruby>[逆文本频率指数][5]<rt>inverse document frequency</rt></ruby>（IDF）。这些指标一定会在本主题系列的其他文章和软件包中提及，因此了解它们会有所帮助。

词袋在许多文档分类应用程序中很有用。然而，在情感分析中，当缺乏情境意识的问题被利用时，事情就可以解决。考虑以下句子：

  * 我们不喜欢这场战争。
  * 我讨厌下雨天，好事是今天是晴天。
  * 这不是生死攸关的问题。

这些短语的情感对于人类口译员来说是有难度的，而且通过严格关注单个词汇的实例，对于机器翻译来说也是困难的。

在 NLP 中也可以使用称为 “n-grams” 的单词分组。一个二元组考虑两个相邻单词组成的组而不是（或除了）单个词袋。这应该可以缓解诸如上述“不喜欢”之类的情况，但由于缺乏语境意思，它仍然是个问题。此外，在上面的第二句中，下半句的情感语境可以被理解为否定前半部分。因此，这种方法中也会丢失上下文线索的空间局部性。从实用角度来看，使问题复杂化的是从给定输入文本中提取的特征的稀疏性。对于一个完整的大型词汇表，每个单词都有一个计数，可以将其视为一个整数向量。大多数文档的向量中都有大量的零计数向量，这给操作增加了不必要的空间和时间复杂度。虽然已经提出了许多用于降低这种复杂性的简便方法，但它仍然是一个问题。

### 词嵌入

<ruby>词嵌入<rt>Word embedding</rt></ruby>是一种分布式表示，它允许具有相似含义的单词具有相似的表示。这是基于使用实值向量来与它们周围相关联。重点在于使用单词的方式，而不仅仅是它们的存在与否。此外，词嵌入的一个巨大实用优势是它们关注于密集向量。通过摆脱具有相应数量的零值向量元素的单词计数模型，词嵌入在时间和存储方面提供了一个更有效的计算范例。

以下是两个优秀的词嵌入方法。

#### Word2vec

第一个是 [Word2vec][6]，它是由 Google 开发的。随着你对 NLP 和情绪分析研究的深入，你可能会看到这种嵌入方法。它要么使用一个<ruby>连续的词袋<rt>continuous bag of words</rt></ruby>（CBOW），要么使用一个连续 skip-gram 模型。在 CBOW 中，一个单词的上下文是在训练中根据围绕它的单词来学习的。连续 skip-gram 学习倾向于围绕给定的单词学习单词。虽然这可能超出了你需要解决的问题，但是如果你曾经面对必须生成自己的词嵌入情况，那么 Word2vec 的作者就提倡使用 CBOW 方法来提高速度并评估频繁的单词，而 skip-gram 方法更适合嵌入稀有单词更重要的嵌入。

#### GloVe

第二个是<ruby>[用于词表示的全局向量][7]<rt>Global Vectors for Word Representation</rt></ruby>（GloVe），它是斯坦福大学开发的。它是 Word2vec 方法的扩展，试图通过将经典的全局文本统计特征提取获得的信息与 Word2vec 确定的本地上下文信息相结合。实际上，在一些应用程序中，GloVe 性能优于 Word2vec，而在另一些应用程序中则不如 Word2vec。最终，用于词嵌入的目标数据集将决定哪种方法最优。因此，最好了解它们的存在性和高级机制，因为你很可能会遇到它们。

#### 创建和使用词嵌入

最后，知道如何获得词嵌入是有用的。在第 2 部分中，你将看到我们通过利用社区中其他人的实质性工作，站到了巨人的肩膀上。这是获取词嵌入的一种方法：即使用现有的经过训练和验证的模型。实际上，有无数的模型适用于英语和其他语言，一定会有一种模型可以满足你的应用程序，让你开箱即用！

如果没有的话，就开发工作而言，另一个极端是培训你自己的独立模型，而不考虑你的应用程序。实质上，你将获得大量标记的训练数据，并可能使用上述方法之一来训练模型。即使这样，你仍然只是在理解你输入文本数据。然后，你需要为你应用程序开发一个特定的模型（例如，分析软件版本控制消息中的情感价值），这反过来又需要自己的时间和精力。

你还可以对针对你的应用程序的数据训练一个词嵌入，虽然这可以减少时间和精力，但这个词嵌入将是特定于应用程序的，这将会降低它的可重用性。

### 可用的工具选项

考虑到所需的大量时间和计算能力，你可能想知道如何才能找到解决问题的方法。的确，开发可靠模型的复杂性可能令人望而生畏。但是，有一个好消息：已经有许多经过验证的模型、工具和软件库可以为我们提供所需的大部分内容。我们将重点关注 [Python][8]，因为它为这些应用程序提供了大量方便的工具。

#### SpaCy

[SpaCy][9] 提供了许多用于解析输入文本数据和提取特征的语言模型。它经过了高度优化，并被誉为同类中最快的库。最棒的是，它是开源的！SpaCy 会执行标识化、词性分类和依赖项注释。它包含了用于执行此功能的词嵌入模型，还有用于为超过 46 种语言的其他特征提取操作。在本系列的第二篇文章中，你将看到它如何用于文本分析和特征提取。

#### vaderSentiment

[vaderSentiment][10] 包提供了积极、消极和中性情绪的衡量标准。正如 [原论文][11] 的标题（《VADER：一个基于规则的社交媒体文本情感分析模型》）所示，这些模型是专门为社交媒体文本数据开发和调整的。VADER 接受了一组完整的人类标记过的数据的训练，包括常见的表情符号、UTF-8 编码的表情符号以及口语术语和缩写（例如 meh、lol、sux）。

对于给定的输入文本数据，vaderSentiment 返回一个极性分数百分比的三元组。它还提供了一个单个的评分标准，称为 *vaderSentiment 复合指标*。这是一个在 `[-1, 1]` 范围内的实值，其中对于分值大于 `0.05` 的情绪被认为是积极的，对于分值小于 `-0.05` 的被认为是消极的，否则为中性。

在[第 2 部分][2]中，你将学习如何使用这些工具为你的设计添加情感分析功能。

--------------------------------------------------------------------------------

via: https://opensource.com/article/19/4/social-media-sentiment-analysis-python

作者：[Michael McCune][a]
选题：[lujun9972][b]
译者：[MjSeven](https://github.com/MjSeven)
校对：[wxy](https://github.com/wxy)

本文由 [LCTT](https://github.com/LCTT/TranslateProject) 原创编译，[Linux中国](https://linux.cn/) 荣誉推出

[a]: https://opensource.com/users/elmiko/users/jschlessman
[b]: https://github.com/lujun9972
[1]: https://opensource.com/sites/default/files/styles/image-full-size/public/lead-images/getting_started_with_python.png?itok=MFEKm3gl (Raspberry Pi and Python)
[2]: https://opensource.com/article/19/4/social-media-sentiment-analysis-python-part-2
[3]: https://en.wikipedia.org/wiki/Bag-of-words_model
[4]: https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency
[5]: https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency
[6]: https://en.wikipedia.org/wiki/Word2vec
[7]: https://en.wikipedia.org/wiki/GloVe_(machine_learning)
[8]: https://www.python.org/
[9]: https://pypi.org/project/spacy/
[10]: https://pypi.org/project/vaderSentiment/
[11]: http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf
