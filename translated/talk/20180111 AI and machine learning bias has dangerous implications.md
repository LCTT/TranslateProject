AI 和机器中暗含的算法偏见是怎样形成的，我们又能通过开源社区做些什么
======

![](https://opensource.com/sites/default/files/styles/image-full-size/public/lead-images/LAW_goodbadugly.png?itok=ZxaimUWU)

图片来源：opensource.com

在我们的世界里，算法无处不在，偏见也是一样。从社会媒体新闻的提供到流式媒体服务的推荐到线上购物，计算机算法，尤其是机器学习算法，已经渗透到我们日常生活的每一个角落。至于偏见，我们只需要参考 2016 年美国大选就可以知道，偏见是怎样在明处与暗处影响着我们的社会。

很难想像，我们经常忽略的一点是这二者的交集：计算机算法中存在的偏见。

与我们大多数人所认为的相反，科技并不是客观的。 AI 算法和它们的决策程序是由它们的研发者塑造的，他们写入的代码，使用的“[训练][1]”数据还有他们对算法进行[应力测试][2] 的过程，都会影响这些算法今后的选择。这意味着研发者的价值观，偏见和人类缺陷都会反映在软件上。如果我只给实验室中的人脸识别算法提供白人的照片，当遇到不是白人照片时，它[不会认为照片中的是人类][3] 。这结论并不意味着 AI 是“愚蠢的”或是“天真的”，它显示的是训练数据的分布偏差：缺乏多种的脸部照片。这会引来非常严重的后果。

这样的例子并不少。全美范围内的[州法院系统][4] 都使用“黑箱子”对罪犯进行宣判。由于训练数据的问题，[这些算法对黑人有偏见][5] ，他们对黑人罪犯会选择更长的服刑期，因此监狱中的种族差异会一直存在。而这些都发生在科技的客观性伪装下，这是“科学的”选择。

美国联邦政府使用机器学习算法来计算福利性支出和各类政府补贴。[但这些算法中的信息][6]，例如它们的创造者和训练信息，都很难找到。这增加了政府工作人员进行不平等补助金分发操作的几率。

算法偏见情况还不止这些。从 Facebook 的新闻算法到医疗系统再到警方使用的相机，我们作为社会的一部分极有可能对这些算法输入各式各样的偏见，性别歧视，仇外思想，社会经济地位歧视，确认偏误等等。这些被输入了偏见的机器会大量生产分配，将种种社会偏见潜藏于科技客观性的面纱之下。

这种状况绝对不能再继续下去了。

在我们对人工智能进行不断开发研究的同时，需要降低它的开发速度，小心仔细地开发。算法偏见的危害已经足够大了。

## 我们能怎样减少算法偏见？

最好的方式是从算法训练的数据开始审查，根据 [Microsoft 的研究者][2] 所说，这方法很有效。

数据分布本身就带有一定的偏见性。编程者手中的美国公民数据分布并不均衡，本地居民的数据多于移民者，富人的数据多于穷人，这是极有可能出现的情况。这种数据的不平均会使 AI 对我们是社会组成得出错误的结论。例如机器学习算法仅仅通过统计分析，就得出“大多数美国人都是富有的白人”这个结论。 

即使男性和女性的样本在训练数据中等量分布，也可能出现偏见的结果。如果训练数据中所有男性的职业都是 CEO，而所有女性的职业都是秘书（即使现实中男性 CEO 的数量要多于女性），AI 也可能得出女性天生不适合做 CEO 的结论。

同样的，大量研究表明，用于执法部门的 AI 在检测新闻中出现的罪犯照片时，结果会 [惊人地偏向][7] 黑人及拉丁美洲裔居民。

在训练数据中存在的偏见还有很多其他形式，不幸的是比这里提到的要多得多。但是训练数据只是审查方式的一种，通过“应力测验”找出人类存在的偏见也同样重要。

如果提供一张印度人的照片，我们自己的相机能够识别吗？在两名同样水平的应聘者中，我们的 AI 是否会倾向于推荐住在市区的应聘者呢？对于情报中本地白人恐怖分子和伊拉克籍恐怖分子，反恐算法会怎样选择呢？急诊室的相机可以调出儿童的病历吗？

这些对于 AI 来说是十分复杂的数据，但我们可以通过多项测试对它们进行定义和传达。

## 为什么开源很适合这项任务？

开源方法和开源技术都有着极大的潜力改变算法偏见。

现代人工智能已经被开源软件占领，TensorFlow、IBM Watson 还有 [scikit-learn][8] 这类的程序包都是开源软件。开源社区已经证明它能够开发出强健的，经得住严酷测试的机器学习工具。同样的，我相信，开源社区也能开发出消除偏见的测试程序，并将其应用于这些软件中。

调试工具如哥伦比亚大学和理海大学推出的 [DeepXplore][9]，增强了 AI 应力测试的强度，同时提高了其操控性。还有 [麻省理工学院的计算机科学和人工智能实验室][10]完成的项目，它开发出敏捷快速的样机研究软件，这些应该会被开源社区采纳。

开源技术也已经证明了其在审查和分类大组数据方面的能力。最明显的体现在开源工具在数据分析市场的占有率上（Weka , Rapid Miner 等等）。应当由开源社区来设计识别数据偏见的工具，已经在网上发布的大量训练数据组比如 [Kaggle][11]也应当使用这种技术进行识别筛选。

开源方法本身十分适合消除偏见程序的设计。内部谈话，私人软件开发及非民主的决策制定引起了很多问题。开源社区能够进行软件公开的谈话，进行大众化，维持好与大众的关系，这对于处理以上问题是十分重要的。如果线上社团，组织和院校能够接受这些开源特质，那么由开源社区进行消除算法偏见的机器设计也会顺利很多。

## 我们怎样才能够参与其中？

教育是一个很重要的环节。我们身边有很多还没意识到算法偏见的人，但算法偏见在立法，社会公正，政策及更多领域产生的影响与他们息息相关。让这些人知道算法偏见是怎样形成的和它们带来的重要影响是很重要的，因为想要改变目前是局面，从我们自身做起是唯一的方法。

对于我们中间那些与人工智能一起工作的人来说，这种沟通尤其重要。不论是人工智能的研发者，警方或是科研人员，当他们为今后设计人工智能时，应当格外意识到现今这种偏见存在的危险性，很明显，想要消除人工智能中存在的偏见，就要从意识到偏见的存在开始。

最后，我们需要围绕 AI 伦理化建立并加强开源社区。不论是需要建立应力实验训练模型，软件工具，或是从千兆字节的训练数据中筛选，现在已经到了我们利用开源方法来应对数字化时代最大的威胁的时间了。

--------------------------------------------------------------------------------

via: https://opensource.com/article/18/1/how-open-source-can-fight-algorithmic-bias

作者：[Justin Sherman][a]
译者：[Valoniakim](https://github.com/Valoniakim)
校对：[校对者ID](https://github.com/校对者ID)

本文由 [LCTT](https://github.com/LCTT/TranslateProject) 原创编译，[Linux中国](https://linux.cn/) 荣誉推出

[a]:https://opensource.com/users/justinsherman
[1]:https://www.crowdflower.com/what-is-training-data/
[2]:https://medium.com/microsoft-design/how-to-recognize-exclusion-in-ai-ec2d6d89f850
[3]:https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms
[4]:https://www.wired.com/2017/04/courts-using-ai-sentence-criminals-must-stop-now/
[5]:https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing
[6]:https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3012499
[7]:https://www.hivlawandpolicy.org/sites/default/files/Race%20and%20Punishment-%20Racial%20Perceptions%20of%20Crime%20and%20Support%20for%20Punitive%20Policies%20%282014%29.pdf
[8]:http://scikit-learn.org/stable/
[9]:https://arxiv.org/pdf/1705.06640.pdf
[10]:https://www.csail.mit.edu/research/understandable-deep-networks
[11]:https://www.kaggle.com/datasets
