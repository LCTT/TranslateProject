[#]: subject: (Test Kubernetes cluster failures and experiments in your terminal)
[#]: via: (https://opensource.com/article/21/6/kubernetes-litmus-chaos)
[#]: author: (Jessica Cherry https://opensource.com/users/cherrybomb)
[#]: collector: (lujun9972)
[#]: translator: ( )
[#]: reviewer: ( )
[#]: publisher: ( )
[#]: url: ( )

Test Kubernetes cluster failures and experiments in your terminal
======
Litmus is an effective tool to cause chaos to test how your system will
respond to failure.
![Science lab with beakers][1]

Do you know how your system will respond to an arbitrary failure? Will your application fail? Will anything survive after a loss? If you're not sure, it's time to see if your system passes the [Litmus][2] test, a detailed way to cause chaos at random with many experiments.

In the first article in this series, I explained [what chaos engineering is][3], and in the second article, I demonstrated how to get your [system's steady state][4] so that you can compare it against a chaos state. This third article will show you how to install and use Litmus to test arbitrary failures and experiments in your Kubernetes cluster. In this walkthrough, I'll use Pop!_OS 20.04, Helm 3, Minikube 1.14.2, and Kubernetes 1.19.

### Configure Minikube

If you haven't already, [install Minikube][5] in whatever way makes sense for your environment. If you have enough resources, I recommend giving your virtual machine a bit more than the default memory and CPU power:


```
$ minikube config set memory 8192
â— Â These changes will take effect upon a minikube delete and then a minikube start
$ minikube config set cpus 6
â— Â These changes will take effect upon a minikube delete and then a minikube start
```

Then start and check your system's status:


```
$ minikube start
ğŸ˜„ Â minikube v1.14.2 on Debian bullseye/sid
ğŸ‰ Â minikube 1.19.0 is available! Download it: <https://github.com/kubernetes/minikube/releases/tag/v1.19.0>
ğŸ’¡ Â To disable this notice, run: 'minikube config set WantUpdateNotification false'

âœ¨ Â Using the docker driver based on user configuration
ğŸ‘ Â Starting control plane node minikube in cluster minikube
ğŸ”¥ Â Creating docker container (CPUs=6, Memory=8192MB) ...
ğŸ³ Â Preparing Kubernetes v1.19.0 on Docker 19.03.8 ...
ğŸ” Â Verifying Kubernetes components...
ğŸŒŸ Â Enabled addons: storage-provisioner, default-storageclass
ğŸ„ Â Done! kubectl is now configured to use "minikube" by default
jess@Athena:~$ minikube status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
```

### Install Litmus

As outlined on [Litmus' homepage][6], the steps to install Litmus are: add your repo to Helm, create your Litmus namespace, then install your chart:


```
$ helm repo add litmuschaos <https://litmuschaos.github.io/litmus-helm/>
"litmuschaos" has been added to your repositories

$ kubectl create ns litmus
namespace/litmus created

$ helm install chaos litmuschaos/litmus --namespace=litmus
NAME: chaos
LAST DEPLOYED: Sun May Â 9 17:05:36 2021
NAMESPACE: litmus
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
```

### Verify the installation

You can run the following commands if you want to verify all the desired components are installed correctly.

Check ifÂ **api-resources**Â for chaos are available:Â 


```
root@demo:~# kubectl api-resources | grep litmus
chaosengines Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  litmuschaos.io Â  Â  Â  Â  Â  Â  Â  Â  true Â  Â  Â  Â  ChaosEngine
chaosexperiments Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  litmuschaos.io Â  Â  Â  Â  Â  Â  Â  Â  true Â  Â  Â  Â  ChaosExperiment
chaosresults Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  litmuschaos.io Â  Â  Â  Â  Â  Â  Â  Â  true Â  Â  Â  Â  ChaosResult
```

Check if the Litmus chaos operator deployment is running successfully:


```
root@demo:~# kubectl get pods -n litmus
NAME Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â READY Â  STATUS Â  Â RESTARTS Â  AGE
litmus-7d998b6568-nnlcd Â  1/1 Â  Â  Running Â  0 Â  Â  Â  Â  Â 106s
```

### Start running chaos experimentsÂ 

With this out of the way, you are good to go! Refer to Litmus'Â [chaos experiment documentation][7]Â to start executing your first experiment.

To confirm your installation is working, check that the pod is up and running correctly:


```
jess@Athena:~$ kubectl get pods -n litmus
NAME Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â READY Â  STATUS Â  Â RESTARTS Â  AGE
litmus-7d6f994d88-2g7wn Â  1/1 Â  Â  Running Â  0 Â  Â  Â  Â  Â 115s
```

Confirm the Custom Resource Definitions (CRDs) are also installed correctly:


```
jess@Athena:~$ kubectl get crds | grep chaos
chaosengines.litmuschaos.io Â  Â  Â  2021-05-09T21:05:33Z
chaosexperiments.litmuschaos.io Â  2021-05-09T21:05:33Z
chaosresults.litmuschaos.io Â  Â  Â  2021-05-09T21:05:33Z
```

Finally, confirm your API resources are also installed:


```
jess@Athena:~$ kubectl api-resources | grep chaos
chaosengines Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  litmuschaos.io Â  Â  Â  Â  Â  Â  Â  Â  true Â  Â  Â  Â  ChaosEngine
chaosexperiments Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  litmuschaos.io Â  Â  Â  Â  Â  Â  Â  Â  true Â  Â  Â  Â  ChaosExperiment
chaosresults Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  litmuschaos.io Â  Â  Â  Â  Â  Â  Â  Â  true Â  Â  Â  Â  ChaosResult
```

That's what I call easy installation and confirmation. The next step is setting up deployments for chaos.

### Prep for destruction

To test for chaos, you need something to test against. Add a new namespace:


```
$ kubectl create namespace more-apps
namespace/more-apps created
```

Then add a deployment to the new namespace:


```
$ kubectl create deployment ghost --namespace more-apps --image=ghost:3.11.0-alpine
deployment.apps/ghost created
```

Finally, scale your deployment up so that you have more than one pod in your deployment to test against:


```
$ kubectl scale deployment/ghost --namespace more-apps --replicas=4
deployment.apps/ghost scaled
```

For Litmus to cause chaos, you need to add an [annotation][8] to your deployment to mark it ready for chaos. Currently, annotations are available for deployments, StatefulSets, and DaemonSets. Add the annotation `chaos=true` to your deployment:


```
$ kubectl annotate deploy/ghost litmuschaos.io/chaos="true" -n more-apps
deployment.apps/ghost annotated
```

Make sure the experiments you will install have the correct permissions to work in the "more-apps" namespace.

Make a new **rbac.yaml** file for the prepper bindings and permissions:


```
`$ touch rbac.yaml`
```

Then add permissions for the generic testing by copying and pasting the code below into your **rbac.yaml** file. These are just basic, minimal permissions to kill pods in your namespace and give Litmus permissions to delete a pod for a namespace you provide:


```
\---
apiVersion: v1
kind: ServiceAccount
metadata:
Â  name: pod-delete-sa
Â  namespace: more-apps
Â  labels:
Â  Â  name: pod-delete-sa
\---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
Â  name: pod-delete-sa
Â  namespace: more-apps
Â  labels:
Â  Â  name: pod-delete-sa
rules:
\- apiGroups: [""]
Â  resources: ["pods","events"]
Â  verbs: ["create","list","get","patch","update","delete","deletecollection"]
\- apiGroups: [""]
Â  resources: ["pods/exec","pods/log","replicationcontrollers"]
Â  verbs: ["create","list","get"]
\- apiGroups: ["batch"]
Â  resources: ["jobs"]
Â  verbs: ["create","list","get","delete","deletecollection"]
\- apiGroups: ["apps"]
Â  resources: ["deployments","statefulsets","daemonsets","replicasets"]
Â  verbs: ["list","get"]
\- apiGroups: ["apps.openshift.io"]
Â  resources: ["deploymentconfigs"]
Â  verbs: ["list","get"]
\- apiGroups: ["argoproj.io"]
Â  resources: ["rollouts"]
Â  verbs: ["list","get"]
\- apiGroups: ["litmuschaos.io"]
Â  resources: ["chaosengines","chaosexperiments","chaosresults"]
Â  verbs: ["create","list","get","patch","update"]
\---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
Â  name: pod-delete-sa
Â  namespace: more-apps
Â  labels:
Â  Â  name: pod-delete-sa
roleRef:
Â  apiGroup: rbac.authorization.k8s.io
Â  kind: Role
Â  name: pod-delete-sa
subjects:
\- kind: ServiceAccount
Â  name: pod-delete-sa
Â  namespace: more-apps
```

Apply the **rbac.yaml** file:


```
$ kubectl apply -f rbac.yaml
serviceaccount/pod-delete-sa created
role.rbac.authorization.k8s.io/pod-delete-sa created
rolebinding.rbac.authorization.k8s.io/pod-delete-sa created
```

The next step is to prepare your chaos engine to delete pods. The chaos engine will connect the experiment you need to your application instance by creating a **chaosengine.yaml** file and copying the information below into the .yaml file. This will connect your experiment to your namespace and the service account with the role bindings you created above.

This chaos engine file only specifies the pod to delete during chaos testing:


```
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosEngine
metadata:
Â  name: moreapps-chaos
Â  namespace: more-apps
spec:
Â  appinfo:
Â  Â  appns: 'more-apps'
Â  Â  applabel: 'app=ghost'
Â  Â  appkind: 'deployment'
Â  # It can be true/false
Â  annotationCheck: 'true'
Â  # It can be active/stop
Â  engineState: 'active'
Â  #ex. values: ns1:name=percona,ns2:run=more-apps
Â  auxiliaryAppInfo: ''
Â  chaosServiceAccount: pod-delete-sa
Â  # It can be delete/retain
Â  jobCleanUpPolicy: 'delete'
Â  experiments:
Â  Â  - name: pod-delete
Â  Â  Â  spec:
Â  Â  Â  Â  components:
Â  Â  Â  Â  Â  env:
Â  Â  Â  Â  Â  Â # set chaos duration (in sec) as desired
Â  Â  Â  Â  Â  Â  - name: TOTAL_CHAOS_DURATION
Â  Â  Â  Â  Â  Â  Â  value: '30'

Â  Â  Â  Â  Â  Â  # set chaos interval (in sec) as desired
Â  Â  Â  Â  Â  Â  - name: CHAOS_INTERVAL
Â  Â  Â  Â  Â  Â  Â  value: '10'

Â  Â  Â  Â  Â  Â  # pod failures without '--force' &amp; default terminationGracePeriodSeconds
Â  Â  Â  Â  Â  Â  - name: FORCE
Â  Â  Â  Â  Â  Â  Â  value: 'false'
```

Don't apply this file until you install the experiments in the next section.

### Add new experiments for causing chaos

Now that you have an entirely new environment with deployments, roles, and the chaos engine to test against, you need some experiments to run. Since Litmus has a large community, you can find some great experiments in the [Chaos Hub][9].

In this walkthrough, I'll use the generic experiment of [killing a pod][10].

Run a kubectl command to install the generic experiments into your cluster. Install this in your `more-apps` namespace; you will see the tests created when you run it:


```
$ kubectl apply -f <https://hub.litmuschaos.io/api/chaos/1.13.3?file=charts/generic/experiments.yaml> -n more-apps
chaosexperiment.litmuschaos.io/pod-network-duplication created
chaosexperiment.litmuschaos.io/node-cpu-hog created
chaosexperiment.litmuschaos.io/node-drain created
chaosexperiment.litmuschaos.io/docker-service-kill created
chaosexperiment.litmuschaos.io/node-taint created
chaosexperiment.litmuschaos.io/pod-autoscaler created
chaosexperiment.litmuschaos.io/pod-network-loss created
chaosexperiment.litmuschaos.io/node-memory-hog created
chaosexperiment.litmuschaos.io/disk-loss created
chaosexperiment.litmuschaos.io/pod-io-stress created
chaosexperiment.litmuschaos.io/pod-network-corruption created
chaosexperiment.litmuschaos.io/container-kill created
chaosexperiment.litmuschaos.io/node-restart created
chaosexperiment.litmuschaos.io/node-io-stress created
chaosexperiment.litmuschaos.io/disk-fill created
chaosexperiment.litmuschaos.io/pod-cpu-hog created
chaosexperiment.litmuschaos.io/pod-network-latency created
chaosexperiment.litmuschaos.io/kubelet-service-kill created
chaosexperiment.litmuschaos.io/k8-pod-delete created
chaosexperiment.litmuschaos.io/pod-delete created
chaosexperiment.litmuschaos.io/node-poweroff created
chaosexperiment.litmuschaos.io/k8-service-kill created
chaosexperiment.litmuschaos.io/pod-memory-hog created
```

Verify the experiments installed correctly:


```
$ kubectl get chaosexperiments -n more-apps
NAME Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â AGE
container-kill Â  Â  Â  Â  Â  Â 72s
disk-fill Â  Â  Â  Â  Â  Â  Â  Â  72s
disk-loss Â  Â  Â  Â  Â  Â  Â  Â  72s
docker-service-kill Â  Â  Â  72s
k8-pod-delete Â  Â  Â  Â  Â  Â  72s
k8-service-kill Â  Â  Â  Â  Â  72s
kubelet-service-kill Â  Â  Â 72s
node-cpu-hog Â  Â  Â  Â  Â  Â  Â 72s
node-drain Â  Â  Â  Â  Â  Â  Â  Â 72s
node-io-stress Â  Â  Â  Â  Â  Â 72s
node-memory-hog Â  Â  Â  Â  Â  72s
node-poweroff Â  Â  Â  Â  Â  Â  72s
node-restart Â  Â  Â  Â  Â  Â  Â 72s
node-taint Â  Â  Â  Â  Â  Â  Â  Â 72s
pod-autoscaler Â  Â  Â  Â  Â  Â 72s
pod-cpu-hog Â  Â  Â  Â  Â  Â  Â  72s
pod-delete Â  Â  Â  Â  Â  Â  Â  Â 72s
pod-io-stress Â  Â  Â  Â  Â  Â  72s
pod-memory-hog Â  Â  Â  Â  Â  Â 72s
pod-network-corruption Â  Â 72s
pod-network-duplication Â  72s
pod-network-latency Â  Â  Â  72s
pod-network-loss Â  Â  Â  Â  Â 72s
```

### Run the experiments

Now that everything is installed and configured, use your **chaosengine.yaml** file to run the pod-deletionÂ experiment you defined. Apply your chaos engine file:


```
$ kubectl apply -f chaosengine.yaml
chaosengine.litmuschaos.io/more-apps-chaos created
```

Confirm the engine started by getting all the pods in your namespace; you should see `pod-delete` being created:


```
$ kubectl get pods -n more-apps
NAME Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â READY Â  STATUS Â  Â  Â  Â  Â  Â  Â RESTARTS Â  AGE
ghost-5bdd4cdcc4-blmtl Â  Â 1/1 Â  Â  Running Â  Â  Â  Â  Â  Â  0 Â  Â  Â  Â  Â 53m
ghost-5bdd4cdcc4-z2lnt Â  Â 1/1 Â  Â  Running Â  Â  Â  Â  Â  Â  0 Â  Â  Â  Â  Â 53m
ghost-5bdd4cdcc4-zlcc9 Â  Â 1/1 Â  Â  Running Â  Â  Â  Â  Â  Â  0 Â  Â  Â  Â  Â 53m
ghost-5bdd4cdcc4-zrs8f Â  Â 1/1 Â  Â  Running Â  Â  Â  Â  Â  Â  0 Â  Â  Â  Â  Â 53m
moreapps-chaos-runner Â  Â  1/1 Â  Â  Running Â  Â  Â  Â  Â  Â  0 Â  Â  Â  Â  Â 17s
pod-delete-e443qx-lxzfx Â  0/1 Â  Â  ContainerCreating Â  0 Â  Â  Â  Â  Â 7s
```

Next, you need to be able to observe your experiments using Litmus. The following command uses the ChaosResult CRD and provides a large amount of output:


```
$ kubectl describe chaosresult moreapps-chaos-pod-delete -n more-apps
Name: Â  Â  Â  Â  moreapps-chaos-pod-delete
Namespace: Â  Â more-apps
Labels: Â  Â  Â  app.kubernetes.io/component=experiment-job
Â  Â  Â  Â  Â  Â  Â  app.kubernetes.io/part-of=litmus
Â  Â  Â  Â  Â  Â  Â  app.kubernetes.io/version=1.13.3
Â  Â  Â  Â  Â  Â  Â  chaosUID=a6c9ab7e-ff07-4703-abe4-43e03b77bd72
Â  Â  Â  Â  Â  Â  Â  controller-uid=601b7330-c6f3-4d9b-90cb-2c761ac0567a
Â  Â  Â  Â  Â  Â  Â  job-name=pod-delete-e443qx
Â  Â  Â  Â  Â  Â  Â  name=moreapps-chaos-pod-delete
Annotations: Â &lt;none&gt;
API Version: Â litmuschaos.io/v1alpha1
Kind: Â  Â  Â  Â  ChaosResult
Metadata:
Â  Creation Timestamp: Â 2021-05-09T22:06:19Z
Â  Generation: Â  Â  Â  Â  Â 2
Â  Managed Fields:
Â  Â  API Version: Â litmuschaos.io/v1alpha1
Â  Â  Fields Type: Â FieldsV1
Â  Â  fieldsV1:
Â  Â  Â  f:metadata:
Â  Â  Â  Â  f:labels:
Â  Â  Â  Â  Â  .:
Â  Â  Â  Â  Â  f:app.kubernetes.io/component:
Â  Â  Â  Â  Â  f:app.kubernetes.io/part-of:
Â  Â  Â  Â  Â  f:app.kubernetes.io/version:
Â  Â  Â  Â  Â  f:chaosUID:
Â  Â  Â  Â  Â  f:controller-uid:
Â  Â  Â  Â  Â  f:job-name:
Â  Â  Â  Â  Â  f:name:
Â  Â  Â  f:spec:
Â  Â  Â  Â  .:
Â  Â  Â  Â  f:engine:
Â  Â  Â  Â  f:experiment:
Â  Â  Â  f:status:
Â  Â  Â  Â  .:
Â  Â  Â  Â  f:experimentStatus:
Â  Â  Â  Â  f:history:
Â  Â  Manager: Â  Â  Â  Â  experiments
Â  Â  Operation: Â  Â  Â  Update
Â  Â  Time: Â  Â  Â  Â  Â  Â 2021-05-09T22:06:53Z
Â  Resource Version: Â 8406
Â  Self Link: Â  Â  Â  Â  /apis/litmuschaos.io/v1alpha1/namespaces/more-apps/chaosresults/moreapps-chaos-pod-delete
Â  UID: Â  Â  Â  Â  Â  Â  Â  08b7e3da-d603-49c7-bac4-3b54eb30aff8
Spec:
Â  Engine: Â  Â  Â moreapps-chaos
Â  Experiment: Â pod-delete
Status:
Â  Experiment Status:
Â  Â  Fail Step: Â  Â  Â  Â  Â  Â  Â  Â  N/A
Â  Â  Phase: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Completed
Â  Â  Probe Success Percentage: Â 100
Â  Â  Verdict: Â  Â  Â  Â  Â  Â  Â  Â  Â  Pass
Â  History:
Â  Â  Failed Runs: Â  0
Â  Â  Passed Runs: Â  1
Â  Â  Stopped Runs: Â 0
Events:
Â  Type Â  Â Reason Â  Age Â  Â From Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Message
Â  ---- Â  Â ------ Â  ---- Â  ---- Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  -------
Â  Normal Â Pass Â  Â  104s Â  pod-delete-e443qx-lxzfx Â experiment: pod-delete, Result: Pass
```

You can see the pass or fail output from your testing as you run the chaos engine definitions.

Congratulations on your first (and hopefully not last) chaos engineering test! Now you have a powerful tool to use and help your environment grow.

### Final thoughts

You might be thinking, "I can't run this manually every time I want to run chaos. How far can I take this, and how can I set it up for the long term?"

Litmus' best part (aside from the Chaos Hub) is its [scheduler][11] function. You can use it to define times and dates, repetitions or sporadic, to run experiments. This is a great tool for detailed admins who have been working with Kubernetes for a while and are ready to create some chaos. I suggest staying up to date on Litmus and how to use this tool for regular chaos engineering. Happy pod hunting!

--------------------------------------------------------------------------------

via: https://opensource.com/article/21/6/kubernetes-litmus-chaos

ä½œè€…ï¼š[Jessica Cherry][a]
é€‰é¢˜ï¼š[lujun9972][b]
è¯‘è€…ï¼š[è¯‘è€…ID](https://github.com/è¯‘è€…ID)
æ ¡å¯¹ï¼š[æ ¡å¯¹è€…ID](https://github.com/æ ¡å¯¹è€…ID)

æœ¬æ–‡ç”± [LCTT](https://github.com/LCTT/TranslateProject) åŸåˆ›ç¼–è¯‘ï¼Œ[Linuxä¸­å›½](https://linux.cn/) è£èª‰æ¨å‡º

[a]: https://opensource.com/users/cherrybomb
[b]: https://github.com/lujun9972
[1]: https://opensource.com/sites/default/files/styles/image-full-size/public/lead-images/science_experiment_beaker_lab.png?itok=plKWRhlU (Science lab with beakers)
[2]: https://github.com/litmuschaos/litmus
[3]: https://opensource.com/article/21/5/11-years-kubernetes-and-chaos
[4]: https://opensource.com/article/21/5/get-your-steady-state-chaos-grafana-and-prometheus
[5]: https://minikube.sigs.k8s.io/docs/start/
[6]: https://litmuschaos.io/
[7]: https://docs.litmuschaos.io
[8]: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
[9]: https://hub.litmuschaos.io/
[10]: https://docs.litmuschaos.io/docs/pod-delete/
[11]: https://docs.litmuschaos.io/docs/scheduling/
